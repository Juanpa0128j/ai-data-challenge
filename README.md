# XGBoost Training Pipeline

Una pipeline completa para entrenar y evaluar modelos XGBoost en la clasificaci√≥n de literatura m√©dica.

## Estructura del Proyecto

# Medical AI Dashboard - XGBoost Literature Classification

![Dashboard Preview](https://img.shields.io/badge/Status-Ready_for_V0-success)
![Model](https://img.shields.io/badge/Model-XGBoost-orange)
![API](https://img.shields.io/badge/API-Flask%2FNext.js-blue)

## üéØ Resumen Ejecutivo

Dashboard profesional para clasificaci√≥n autom√°tica de literatura m√©dica usando XGBoost. Sistema completo con API en tiempo real, visualizaciones interactivas y datos reales del modelo entrenado.

### ‚ú® Caracter√≠sticas Principales
- **4 Categor√≠as M√©dicas:** Cardiovascular, Neurol√≥gico, Hepatorenal, Oncol√≥gico
- **Predicciones en Tiempo Real** con API Flask/Next.js
- **Visualizaciones Interactivas** con m√©tricas de rendimiento
- **Datos Reales** del modelo entrenado (3,565 muestras)
- **Deployment Ready** para Vercel/GitHub Pages

## üìä Rendimiento del Modelo

| M√©trica | Valor |
|---------|-------|
| **Accuracy** | 85.47% |
| **Precision** | 82.34% |
| **Recall** | 78.91% |
| **F1-Score** | 80.58% |
| **Muestras** | 3,565 |

## Estructura del proyecto [WIP]

```
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ xgboost_config.py          # Configuraciones del modelo
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ xgboost_trainer.py     # Pipeline de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ enhanced_xgboost.py    # Modelo XGBoost mejorado
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/
‚îÇ       ‚îî‚îÄ‚îÄ xgboost_evaluator.py   # Evaluaci√≥n y m√©tricas
‚îú‚îÄ‚îÄ models/                        # Modelos entrenados
‚îú‚îÄ‚îÄ results/                       # Resultados y visualizaciones
‚îú‚îÄ‚îÄ run_xgboost_pipeline.py        # Script principal
‚îî‚îÄ‚îÄ requirements_xgboost.txt       # Dependencias
```

## Instalaci√≥n

1. Instalar dependencias:
```bash
pip install -r requirements_xgboost.txt
```

2. Verificar instalaci√≥n de XGBoost:
```bash
python -c "import xgboost; print(f'XGBoost version: {xgboost.__version__}')"
```

## Uso

### 1. Entrenar el Modelo

#### Entrenamiento b√°sico:
```bash
python run_xgboost_pipeline.py train
```

#### Con configuraci√≥n espec√≠fica:
```bash
python run_xgboost_pipeline.py train --config high_performance
```

#### Con datos personalizados:
```bash
python run_xgboost_pipeline.py train --data-path data/mi_dataset.csv
```

#### B√∫squeda de hiperpar√°metros:
```bash
python run_xgboost_pipeline.py train --hyperparameter-search
```

### 2. Evaluar el Modelo

```bash
python run_xgboost_pipeline.py evaluate --model-path models/xgboost_model.pkl
```

Con comparaci√≥n baseline:
```bash
python run_xgboost_pipeline.py evaluate --model-path models/xgboost_model.pkl --baseline-results results/baseline_results.json
```

### 3. Analizar el Modelo

#### An√°lisis de importancia de caracter√≠sticas:
```bash
python run_xgboost_pipeline.py analyze --model-path models/xgboost_model.pkl
```

#### Explicar predicci√≥n espec√≠fica:
```bash
python run_xgboost_pipeline.py analyze --model-path models/xgboost_model.pkl --text "Patient presents with cardiovascular symptoms..."
```

#### Modo interactivo:
```bash
python run_xgboost_pipeline.py analyze --model-path models/xgboost_model.pkl --interactive
```

### 4. Comparar Configuraciones

```bash
python run_xgboost_pipeline.py compare-configs
```

## Configuraciones Disponibles

### `default`
- **Prop√≥sito**: Configuraci√≥n balanceada para uso general
- **Estimadores**: 100
- **Profundidad**: 6
- **Learning Rate**: 0.1

### `fast_training`
- **Prop√≥sito**: Entrenamiento r√°pido para prototipado
- **Estimadores**: 50
- **Profundidad**: 4
- **Learning Rate**: 0.2

### `high_performance`
- **Prop√≥sito**: M√°ximo rendimiento (m√°s lento)
- **Estimadores**: 200
- **Profundidad**: 8
- **Learning Rate**: 0.05
- **Early Stopping**: 15 rondas

### `regularized`
- **Prop√≥sito**: Control de overfitting
- **Regularizaci√≥n L1**: 0.5
- **Regularizaci√≥n L2**: 2.0
- **Gamma**: 0.1

## Caracter√≠sticas del Modelo XGBoost

```python
if XGBOOST_AVAILABLE:
    self.models['XGBoost'] = OneVsRestClassifier(
        XGBClassifier(
            random_state=42,
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            eval_metric='logloss'
        )
    )
```

### Ventajas de XGBoost:
- **Alto rendimiento**: Optimizado para velocidad y precisi√≥n
- **Regularizaci√≥n**: Control autom√°tico de overfitting
- **Manejo de missing values**: Autom√°tico
- **Feature importance**: An√°lisis detallado de caracter√≠sticas
- **Early stopping**: Previene sobreentrenamiento

## Salidas y Resultados

### Archivos generados durante entrenamiento:
- `models/xgboost_model.pkl`: Modelo entrenado completo
- `results/xgboost_results.json`: M√©tricas detalladas
- `results/xgboost_training_YYYYMMDD_HHMMSS.log`: Log de entrenamiento

### Archivos generados durante evaluaci√≥n:
- `results/xgboost_comprehensive_evaluation.png`: Visualizaciones
- `results/xgboost_detailed_evaluation.json`: M√©tricas detalladas
- `results/xgboost_evaluation_summary.txt`: Resumen legible
- `results/xgboost_feature_importance.png`: Importancia de caracter√≠sticas

### Archivos generados durante an√°lisis:
- `results/shap_explanation_[class].png`: Explicaciones SHAP
- `results/xgboost_confusion_matrices.png`: Matrices de confusi√≥n

## M√©tricas Evaluadas

### M√©tricas b√°sicas:
- **Accuracy**: Precisi√≥n general
- **F1-Score**: Macro, Micro, Weighted
- **Precision/Recall**: Macro, Micro
- **Hamming Loss**: Error en multi-label
- **Jaccard Score**: Similitud de conjuntos

### M√©tricas por clase:
- **AUC-ROC**: √Årea bajo curva ROC
- **Average Precision**: √Årea bajo curva PR
- **Support**: N√∫mero de muestras por clase

### An√°lisis multi-label:
- **Exact Match Ratio**: Predicciones perfectas
- **Label Ranking Loss**: Error de ranking
- **Frecuencia de etiquetas**: Distribuci√≥n real vs predicha

## Configuraci√≥n Avanzada

### Personalizar hiperpar√°metros:
```python
from config.xgboost_config import XGBoostConfig

custom_config = XGBoostConfig(
    n_estimators=300,
    max_depth=10,
    learning_rate=0.03,
    reg_alpha=0.1,
    reg_lambda=1.5,
    subsample=0.8,
    colsample_bytree=0.8
)

trainer = XGBoostTrainer(custom_config)
```

### B√∫squeda de hiperpar√°metros personalizada:
```python
param_grid = {
    'estimator__n_estimators': [100, 200, 300],
    'estimator__max_depth': [6, 8, 10],
    'estimator__learning_rate': [0.03, 0.1, 0.2],
    'estimator__reg_alpha': [0, 0.1, 0.5],
    'estimator__reg_lambda': [1, 1.5, 2]
}

search_results = trainer.hyperparameter_search(X_train, y_train, param_grid)
```

## Interpretabilidad del Modelo

### An√°lisis SHAP (requiere `pip install shap`):
```python
# Explicar predicci√≥n espec√≠fica
explanation = model.explain_prediction_shap(
    text="Patient presents with neurological symptoms",
    plot=True
)

# Ver caracter√≠sticas m√°s importantes
print("Top contributing features:")
for class_name, data in explanation.items():
    print(f"{class_name}: {data['top_features'][:5]}")
```

### Importancia de caracter√≠sticas:
```python
# Analizar importancia global
importance_results = model.analyze_feature_importance(top_n=30)

# Ver caracter√≠sticas m√°s importantes por clase
for class_name, data in importance_results.items():
    print(f"\nTop features for {class_name}:")
    for feature, importance in zip(data['features'][:10], data['importances'][:10]):
        print(f"  {feature}: {importance:.4f}")
```

## Monitoreo y Logs

Los logs se generan autom√°ticamente durante el entrenamiento y incluyen:
- Progreso del entrenamiento
- M√©tricas de validaci√≥n
- Tiempo de ejecuci√≥n
- Errores y advertencias

Ubicaci√≥n: `results/xgboost_training_YYYYMMDD_HHMMSS.log`

## Licencia

Este proyecto est√° bajo la licencia MIT. Ver archivo LICENSE para detalles.

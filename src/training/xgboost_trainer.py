"""
XGBoost Training Pipeline for Medical Literature Classification
==============================================================

Complete training pipeline for XGBoost model including data preprocessing,
model training, validation, and evaluation.

Authors: Juan Pablo MejÃ­a, Samuel CastaÃ±o, Mateo Builes
"""

import os
import sys
import pandas as pd
import numpy as np
import joblib
import json
from datetime import datetime
from typing import Tuple, Dict, Any, Optional
import logging

# Add parent directories to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.multiclass import OneVsRestClassifier
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (
    classification_report, f1_score, accuracy_score, 
    precision_score, recall_score, roc_auc_score
)
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

# Try to import XGBoost
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("XGBoost not available. Please install with: pip install xgboost")

from config.xgboost_config import XGBoostConfig, get_config


class XGBoostTrainer:
    """
    Training pipeline for XGBoost model on medical literature classification.
    """
    
    def __init__(self, config: Optional[XGBoostConfig] = None):
        """
        Initialize the XGBoost trainer.
        
        Parameters:
        -----------
        config : XGBoostConfig, optional
            Configuration object. If None, uses default configuration.
        """
        self.config = config or get_config('default')
        self.model = None
        self.vectorizer = None
        self.label_binarizer = None
        self.classes_ = ['cardiovascular', 'neurological', 'hepatorenal', 'oncological']
        
        # Setup logging
        self._setup_logging()
        
        # Check XGBoost availability
        if not XGBOOST_AVAILABLE:
            raise ImportError("XGBoost is required but not installed. Install with: pip install xgboost")
    
    def _setup_logging(self):
        """Setup logging configuration."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'results/xgboost_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('XGBoostTrainer')
    
    def load_data(self, data_path: Optional[str] = None) -> pd.DataFrame:
        """
        Load the training data.
        
        Parameters:
        -----------
        data_path : str, optional
            Path to the data file. If None, uses config path.
        
        Returns:
        --------
        pd.DataFrame
            Loaded dataframe
        """
        path = data_path or self.config.data_path
        self.logger.info(f"Loading data from: {path}")
        
        try:
            # Try different separators
            try:
                df = pd.read_csv(path, sep=';')
                self.logger.info("Data loaded with semicolon separator")
            except:
                df = pd.read_csv(path, sep=',')
                self.logger.info("Data loaded with comma separator")
            
            self.logger.info(f"Data loaded successfully. Shape: {df.shape}")
            self.logger.info(f"Columns: {list(df.columns)}")
            return df
        except Exception as e:
            self.logger.error(f"Error loading data: {str(e)}")
            raise
    
    def preprocess_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        Preprocess the data for XGBoost training.
        
        Parameters:
        -----------
        df : pd.DataFrame
            Input dataframe
        
        Returns:
        --------
        Tuple[np.ndarray, np.ndarray]
            Processed features and labels
        """
        self.logger.info("Preprocessing data...")
        self.logger.info(f"Input DataFrame columns: {df.columns.tolist()}")
        
        # Handle different column structures
        if 'title_and_abstract' in df.columns:
            # Original structure
            texts = df['title_and_abstract'].fillna('').astype(str)
        elif 'title' in df.columns and 'abstract' in df.columns:
            # Challenge data structure: combine title and abstract
            texts = (df['title'].fillna('') + ' ' + df['abstract'].fillna('')).astype(str)
            self.logger.info("Combined title and abstract columns")
        elif 'text' in df.columns:
            # Single text column
            texts = df['text'].fillna('').astype(str)
        else:
            raise ValueError(f"No suitable text column found. Available columns: {df.columns.tolist()}")
        
        # Define custom stop words
        custom_stop_words = ['organ', 'interplay', 'connections', 'insights', 'perspective', 
                             'improves', 'patterns', 'secrets', 'markers', 'symptons']
        combined_stop_words = list(ENGLISH_STOP_WORDS.union(custom_stop_words))

        # Vectorize text using TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=self.config.max_features,
            ngram_range=(1, 2),
            stop_words=combined_stop_words,
            lowercase=True,
            strip_accents='ascii'
        )
        
        X = self.vectorizer.fit_transform(texts).toarray()
        self.logger.info(f"Text vectorization completed. Feature shape: {X.shape}")
        
        # Prepare labels - handle different label structures
        labels = []
        
        if 'group' in df.columns:
            # Challenge data structure: group column with pipe-separated values
            self.logger.info("Processing 'group' column with pipe-separated labels")
            for _, row in df.iterrows():
                row_labels = []
                if pd.notna(row['group']):
                    # Split by pipe and clean up
                    group_labels = [label.strip() for label in str(row['group']).split('|')]
                    # Only keep labels that are in our predefined classes
                    for label in group_labels:
                        if label in self.classes_:
                            row_labels.append(label)
                labels.append(row_labels)
        else:
            # Original structure: separate binary columns for each class
            self.logger.info("Processing separate binary columns for each class")
            for _, row in df.iterrows():
                row_labels = []
                for class_name in self.classes_:
                    if class_name in df.columns and row[class_name] == 1:
                        row_labels.append(class_name)
                labels.append(row_labels)
        
        # Binarize labels for multi-label classification
        self.label_binarizer = MultiLabelBinarizer(classes=self.classes_)
        y = self.label_binarizer.fit_transform(labels)
        
        self.logger.info(f"Label preparation completed. Label shape: {y.shape}")
        self.logger.info(f"Classes: {self.classes_}")
        self.logger.info(f"Label distribution: {dict(zip(self.classes_, np.sum(y, axis=0)))}")
        
        # Show some sample labels for debugging
        sample_labels = labels[:5]
        self.logger.info(f"Sample labels: {sample_labels}")
        
        return X, y
    
    def create_model(self) -> OneVsRestClassifier:
        """
        Create the XGBoost model with OneVsRestClassifier wrapper.
        
        Returns:
        --------
        OneVsRestClassifier
            Configured XGBoost model
        """
        self.logger.info("Creating XGBoost model...")
        
        xgb_params = self.config.get_model_params()
        base_model = XGBClassifier(**xgb_params)
        
        model = OneVsRestClassifier(base_model, n_jobs=1)  # Set n_jobs=1 for OneVsRest to avoid conflicts
        
        self.logger.info(f"XGBoost model created with parameters: {xgb_params}")
        return model
    
    def train_model(
        self, 
        X_train: np.ndarray, 
        y_train: np.ndarray,
        X_val: Optional[np.ndarray] = None,
        y_val: Optional[np.ndarray] = None
    ) -> OneVsRestClassifier:
        """
        Train the XGBoost model.
        
        Parameters:
        -----------
        X_train : np.ndarray
            Training features
        y_train : np.ndarray
            Training labels
        X_val : np.ndarray, optional
            Validation features
        y_val : np.ndarray, optional
            Validation labels
        
        Returns:
        --------
        OneVsRestClassifier
            Trained model
        """
        self.logger.info("Starting model training...")
        start_time = datetime.now()
        
        self.model = self.create_model()
        
        # Train the model
        if X_val is not None and y_val is not None:
            # TODO: Implement early stopping for OneVsRestClassifier
            # For now, just train normally
            self.model.fit(X_train, y_train)
        else:
            self.model.fit(X_train, y_train)
        
        training_time = datetime.now() - start_time
        self.logger.info(f"Training completed in: {training_time}")
        
        return self.model
    
    def evaluate_model(
        self, 
        X_test: np.ndarray, 
        y_test: np.ndarray
    ) -> Dict[str, Any]:
        """
        Evaluate the trained model.
        
        Parameters:
        -----------
        X_test : np.ndarray
            Test features
        y_test : np.ndarray
            Test labels
        
        Returns:
        --------
        Dict[str, Any]
            Evaluation metrics
        """
        if self.model is None:
            raise ValueError("Model not trained yet. Call train_model() first.")
        
        self.logger.info("Evaluating model...")
        
        # Make predictions - using try/catch for compatibility issues
        try:
            y_pred = self.model.predict(X_test)
            y_pred_proba = self.model.predict_proba(X_test)
        except AttributeError as e:
            self.logger.error(f"Prediction error: {str(e)}")
            # Fallback: make predictions for each estimator individually
            y_pred = np.zeros_like(y_test)
            y_pred_proba = np.zeros_like(y_test, dtype=float)
            
            for i, estimator in enumerate(self.model.estimators_):
                try:
                    pred_single = estimator.predict(X_test)
                    pred_proba_single = estimator.predict_proba(X_test)[:, 1]  # Get positive class probability
                    y_pred[:, i] = pred_single
                    y_pred_proba[:, i] = pred_proba_single
                except Exception as e_single:
                    self.logger.warning(f"Error predicting for estimator {i}: {str(e_single)}")
                    # Use random predictions as fallback
                    y_pred[:, i] = np.random.randint(0, 2, size=X_test.shape[0])
                    y_pred_proba[:, i] = np.random.random(size=X_test.shape[0])
        
        # Calculate metrics
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'f1_macro': f1_score(y_test, y_pred, average='macro'),
            'f1_micro': f1_score(y_test, y_pred, average='micro'),
            'f1_weighted': f1_score(y_test, y_pred, average='weighted'),
            'precision_macro': precision_score(y_test, y_pred, average='macro', zero_division=0),
            'precision_micro': precision_score(y_test, y_pred, average='micro', zero_division=0),
            'recall_macro': recall_score(y_test, y_pred, average='macro', zero_division=0),
            'recall_micro': recall_score(y_test, y_pred, average='micro', zero_division=0),
        }
        
        # Calculate AUC-ROC if possible
        try:
            metrics['roc_auc_macro'] = roc_auc_score(y_test, y_pred_proba, average='macro')
            metrics['roc_auc_micro'] = roc_auc_score(y_test, y_pred_proba, average='micro')
        except Exception as e:
            self.logger.warning(f"Could not calculate ROC-AUC: {str(e)}")
        
        # Per-class metrics
        try:
            class_report = classification_report(
                y_test, y_pred, 
                target_names=self.classes_,
                output_dict=True,
                zero_division=0
            )
            metrics['classification_report'] = class_report
        except Exception as e:
            self.logger.warning(f"Could not generate classification report: {str(e)}")
            metrics['classification_report'] = {}
        
        self.logger.info("Evaluation completed")
        for metric, value in metrics.items():
            if isinstance(value, (int, float)):
                self.logger.info(f"{metric}: {value:.4f}")
        
        return metrics
    
    def save_model(self, filepath: Optional[str] = None) -> None:
        """
        Save the trained model and preprocessors.
        
        Parameters:
        -----------
        filepath : str, optional
            Path to save the model. If None, uses config path.
        """
        if self.model is None:
            raise ValueError("Model not trained yet. Call train_model() first.")
        
        save_path = filepath or self.config.model_save_path
        
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # Save model and preprocessors
        model_data = {
            'model': self.model,
            'vectorizer': self.vectorizer,
            'label_binarizer': self.label_binarizer,
            'config': self.config.to_dict(),
            'classes': self.classes_,
            'timestamp': datetime.now().isoformat()
        }
        
        joblib.dump(model_data, save_path)
        self.logger.info(f"Model saved to: {save_path}")
    
    def cross_validate(
        self, 
        X: np.ndarray, 
        y: np.ndarray, 
        cv: int = 5
    ) -> Dict[str, float]:
        """
        Perform cross-validation.
        
        Parameters:
        -----------
        X : np.ndarray
            Features
        y : np.ndarray
            Labels
        cv : int
            Number of cross-validation folds
        
        Returns:
        --------
        Dict[str, float]
            Cross-validation scores
        """
        self.logger.info(f"Performing {cv}-fold cross-validation...")
        
        model = self.create_model()
        
        # Perform cross-validation with different scoring metrics
        scoring_metrics = ['f1_macro', 'f1_micro', 'accuracy']
        cv_results = {}
        
        for metric in scoring_metrics:
            scores = cross_val_score(model, X, y, cv=cv, scoring=metric, n_jobs=1)
            cv_results[f'{metric}_mean'] = np.mean(scores)
            cv_results[f'{metric}_std'] = np.std(scores)
            
            self.logger.info(f"{metric}: {np.mean(scores):.4f} (+/- {np.std(scores) * 2:.4f})")
        
        return cv_results
    
    def hyperparameter_search(
        self, 
        X_train: np.ndarray, 
        y_train: np.ndarray,
        param_grid: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Perform hyperparameter search using GridSearchCV.
        
        Parameters:
        -----------
        X_train : np.ndarray
            Training features
        y_train : np.ndarray
            Training labels
        param_grid : Dict[str, Any], optional
            Parameter grid for search
        
        Returns:
        --------
        Dict[str, Any]
            Best parameters and scores
        """
        if param_grid is None:
            param_grid = {
                'estimator__n_estimators': [50, 100, 200, 300, 400],
                'estimator__max_depth': [4, 6, 8, 10, 12],
                'estimator__learning_rate': [0.05, 0.1, 0.2, 0.3, 0.4]
            }
        
        self.logger.info("Starting hyperparameter search...")
        self.logger.info(f"Parameter grid: {param_grid}")
        
        base_model = self.create_model()
        
        grid_search = GridSearchCV(
            base_model,
            param_grid,
            cv=3,
            scoring='f1_macro',
            n_jobs=1,
            verbose=1
        )
        
        grid_search.fit(X_train, y_train)
        
        results = {
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'cv_results': grid_search.cv_results_
        }
        
        self.logger.info(f"Best parameters: {grid_search.best_params_}")
        self.logger.info(f"Best cross-validation score: {grid_search.best_score_:.4f}")
        
        return results
    
    def run_full_pipeline(
        self, 
        data_path: Optional[str] = None,
        perform_cv: bool = True,
        save_results: bool = True
    ) -> Dict[str, Any]:
        """
        Run the complete training pipeline.
        
        Parameters:
        -----------
        data_path : str, optional
            Path to the data file
        perform_cv : bool
            Whether to perform cross-validation
        save_results : bool
            Whether to save results and model
        
        Returns:
        --------
        Dict[str, Any]
            Complete results dictionary
        """
        self.logger.info("Starting XGBoost training pipeline...")
        pipeline_start = datetime.now()
        
        # Load and preprocess data
        df = self.load_data(data_path)
        X, y = self.preprocess_data(df)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, 
            test_size=self.config.test_size,
            random_state=self.config.random_state,
            stratify=y
        )
        
        # Train model
        self.train_model(X_train, y_train)
        
        # Evaluate model
        test_metrics = self.evaluate_model(X_test, y_test)
        
        # Cross-validation
        cv_results = None
        if perform_cv:
            cv_results = self.cross_validate(X_train, y_train)
        
        # Compile results
        results = {
            'config': self.config.to_dict(),
            'data_shape': {
                'total_samples': len(df),
                'features': X.shape[1],
                'train_samples': X_train.shape[0],
                'test_samples': X_test.shape[0]
            },
            'test_metrics': test_metrics,
            'cv_results': cv_results,
            'training_time': str(datetime.now() - pipeline_start),
            'timestamp': datetime.now().isoformat()
        }
        
        # Save results and model
        if save_results:
            self.save_model()
            self._save_results(results)
        
        pipeline_time = datetime.now() - pipeline_start
        self.logger.info(f"Pipeline completed in: {pipeline_time}")
        
        return results
    
    def _save_results(self, results: Dict[str, Any]) -> None:
        """Save results to JSON file."""
        os.makedirs(os.path.dirname(self.config.results_path), exist_ok=True)
        
        with open(self.config.results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, default=str)
        
        self.logger.info(f"Results saved to: {self.config.results_path}")


def main():
    """Main function to run the training pipeline."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Train XGBoost model for medical literature classification')
    parser.add_argument('--config', type=str, default='default', 
                       help='Configuration to use (default, fast_training, high_performance, regularized)')
    parser.add_argument('--data-path', type=str, 
                       help='Path to the data file (overrides config)')
    parser.add_argument('--no-cv', action='store_true', 
                       help='Skip cross-validation')
    parser.add_argument('--hyperparameter-search', action='store_true',
                       help='Perform hyperparameter search')
    
    args = parser.parse_args()
    
    # Load configuration
    config = get_config(args.config)
    if args.data_path:
        config.data_path = args.data_path
    
    # Initialize trainer
    trainer = XGBoostTrainer(config)
    
    # Perform hyperparameter search if requested
    if args.hyperparameter_search:
        print("Performing hyperparameter search...")
        df = trainer.load_data()
        X, y = trainer.preprocess_data(df)
        X_train, _, y_train, _ = train_test_split(
            X, y, test_size=config.test_size, random_state=config.random_state
        )
        search_results = trainer.hyperparameter_search(X_train, y_train)
        print(f"Best parameters: {search_results['best_params']}")
        return
    
    # Run full pipeline
    results = trainer.run_full_pipeline(
        data_path=args.data_path,
        perform_cv=not args.no_cv
    )
    
    print("\n" + "="*50)
    print("TRAINING COMPLETED!")
    print("="*50)
    print(f"F1 Score (Macro): {results['test_metrics']['f1_macro']:.4f}")
    print(f"F1 Score (Micro): {results['test_metrics']['f1_micro']:.4f}")
    print(f"Accuracy: {results['test_metrics']['accuracy']:.4f}")


if __name__ == "__main__":
    main()
